{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory\n",
    "current_dir = !pwd\n",
    "print(current_dir)\n",
    "\n",
    "file_list = !ls\n",
    "print(file_list)\n",
    "\n",
    "#All files in ds\n",
    "print(dataset_list)\n",
    "\n",
    "\n",
    "#DF\n",
    "import pandas as pd\n",
    "\n",
    "# Read in `road-accidents.csv`\n",
    "car_acc = pd.read_csv(\"datasets/road-accidents.csv\", comment = \"#\", sep = \"|\")\n",
    "\n",
    "# create tuple\n",
    "rows_and_cols = car_acc.shape\n",
    "print('There are {} rows and {} columns.\\n'.format(\n",
    "    rows_and_cols[0], rows_and_cols[1]))\n",
    "#DataFrame\n",
    "car_acc_information = car_acc.info()\n",
    "print(car_acc_information)\n",
    "#overview of data\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sum_stat_car = car_acc.describe()\n",
    "print(sum_stat_car)\n",
    "\n",
    "#pairwise scatter plot to explore the data\n",
    "\n",
    "sns.pairplot(car_acc)\n",
    "#association of features and accidents\n",
    "#correlation coefficent for all columns\n",
    "corr_columns = car_acc.corr()\n",
    "corr_columns\n",
    "#linear regression\n",
    "from sklearn import linear_model\n",
    "\n",
    "#features and target DataFrames\n",
    "features = car_acc[['perc_fatl_speed', 'perc_fatl_alcohol', 'perc_fatl_1st_time']]\n",
    "target = car_acc['drvr_fatl_col_bmiles']\n",
    "\n",
    "# linear regression object\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# Fit model\n",
    "reg.fit(features,target)\n",
    "# regression coefficients\n",
    "fit_coef = reg.coef_\n",
    "fit_coef\n",
    "\n",
    "\n",
    "# Standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Import the PCA class and fit data\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# bar plot the proportion of variance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(1, pca.n_components_ + 1),  pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal #')\n",
    "plt.ylabel('Proportion of variance')\n",
    "plt.xticks([1, 2, 3])\n",
    "\n",
    "# cumulative proportion of variance from first two principal components\n",
    "two_first_comp_var_exp = pca.explained_variance_ratio_.cumsum()[1]\n",
    "print(\"The cumulative variance of the first two principal components is {}\".format(\n",
    "    round(two_first_comp_var_exp, 5)))\n",
    "#visualize the principal components\n",
    "\n",
    "# transform the scaled features using two principal components\n",
    "pca = PCA(n_components=2)\n",
    "p_comps = pca.fit_transform(features_scaled)\n",
    "\n",
    "# extract the first and second component to use for the scatter plot\n",
    "p_comp1 = p_comps[:, 0]\n",
    "p_comp2 = p_comps[:, 1]\n",
    "\n",
    "# scatter plot for the first two principal components\n",
    "plt.scatter(p_comp1, p_comp2)\n",
    "# finding clusters of data using KMeans\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=8)\n",
    "    km.fit(features_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "    \n",
    "# line plot for results\n",
    "plt.plot(ks, inertias, marker='o')\n",
    "\n",
    "# create new KMeans object\n",
    "\n",
    "km = KMeans(n_clusters = 3, random_state = 8)\n",
    "\n",
    "# fit data\n",
    "\n",
    "km.fit(features_scaled)\n",
    "\n",
    "# scatter plot for the  2 components\n",
    "\n",
    "plt.scatter(p_comps[:, 0], p_comps[:, 1], c=km.labels_)\n",
    "\n",
    "#create visual that shows the difference between each cluster\n",
    "# new column with the labels from the KMeans\n",
    "\n",
    "car_acc['cluster'] = km.labels_\n",
    "\n",
    "# creating a long format df to showcase a violin plot\n",
    "\n",
    "melt_car = pd.melt(car_acc, id_vars='cluster', var_name='measurement', value_name='percent',\n",
    "                   value_vars=['perc_fatl_speed', 'perc_fatl_alcohol', 'perc_fatl_1st_time'])\n",
    "sns.violinplot(y='measurement', x='percent', data=melt_car, hue='cluster')\n",
    "\n",
    "# read in new dataset and merge with previous df\n",
    "miles_driven = pd.read_csv('datasets/miles-driven.csv', sep='|')\n",
    "car_acc_miles = car_acc.merge(miles_driven, on='state')\n",
    "\n",
    "# make a new column for the number of drivers involved in fatal accidents amd show results in a barplot\n",
    "car_acc_miles['num_drvr_fatl_col'] = car_acc_miles['drvr_fatl_col_bmiles'] * car_acc_miles['million_miles_annually'] / 1000\n",
    "\n",
    "sns.barplot(x='cluster', y='num_drvr_fatl_col', data=car_acc_miles, estimator=sum, ci=None)\n",
    "\n",
    "# calculate count,mean,sum of the number of states in each cluster for the col 'num_drvr_fatl_col'\n",
    "count_mean_sum = car_acc_miles.groupby('cluster')['num_drvr_fatl_col'].agg(['count', 'mean', 'sum'])\n",
    "count_mean_sum\n",
    "#understanding the data derived from the barplot as col 0 being the most significant cluster "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
